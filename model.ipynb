{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import DatasetFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.dataset_folder = DatasetFolder(root=root, loader=CNNDataset._npy_loader, extensions=('_mfcc.npy',))\n",
    "        self.len_ = len(self.dataset_folder)\n",
    "        self.folder_to_index = self.dataset_folder.class_to_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def _npy_loader(path):\n",
    "        mfcc = np.load(path)\n",
    "        non_mfcc_file_path = path.replace('mfcc', 'other')\n",
    "        non_mfcc = np.load(non_mfcc_file_path)\n",
    "\n",
    "        # in_channels x height x width\n",
    "        assert mfcc.shape == (3, 13, 30)\n",
    "        assert non_mfcc.shape == (18, )\n",
    "\n",
    "        mfcc = torch.from_numpy(mfcc).float()\n",
    "        non_mfcc = torch.from_numpy(non_mfcc).float()\n",
    "\n",
    "        return mfcc, non_mfcc, path\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.dataset_folder[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                      padding=padding, stride=stride),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                      padding=padding, stride=stride),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.network(x)\n",
    "        out = out + x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNStressNet(nn.Module):\n",
    "\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.loss_layer = nn.CrossEntropyLoss(reduction=reduction)\n",
    "        self.cnn_network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=(3 - 1)//2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=(3 - 1)//2, stride=2),\n",
    "            ResBlock(in_channels=32, out_channels=32, kernel_size=3),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=(3 - 1) // 2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=(0, (3 - 1) // 2), stride=2),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=(1, 4))\n",
    "        )\n",
    "\n",
    "        self.dnn_network = nn.Sequential(\n",
    "            nn.Linear(18, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, mfcc, non_mfcc):\n",
    "        n = mfcc.shape[0]\n",
    "        cnn_out = self.cnn_network(mfcc)\n",
    "        cnn_out = cnn_out.reshape(n, 64)\n",
    "\n",
    "        dnn_out = self.dnn_network(non_mfcc)\n",
    "\n",
    "        out = torch.cat([cnn_out, dnn_out], dim=1)\n",
    "        out = self.fully_connected(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def loss(self, predictions, labels):\n",
    "        loss_val = self.loss_layer(predictions, labels)\n",
    "        return loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_files(files):\n",
    "    for f in files:\n",
    "        return os.remove(f)\n",
    "\n",
    "\n",
    "def assert_dir_exits(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def save_model(model, epoch, out_path):\n",
    "    assert_dir_exits(out_path)\n",
    "    chk_files = glob.glob(out_path + '/' + '*.pth')\n",
    "    _remove_files(chk_files)\n",
    "    torch.save(model.state_dict(), out_path + '/' + str(epoch) + '.pth')\n",
    "    print('model saved for epoch: {}'.format(epoch))\n",
    "\n",
    "\n",
    "def save_objects(obj, epoch, out_path):\n",
    "    assert_dir_exits(out_path)\n",
    "    dat_files = glob.glob(out_path + '/' + '*.dat')\n",
    "    _remove_files(dat_files)\n",
    "    # object should be tuple\n",
    "    with open(out_path + '/' + str(epoch) + '.dat', 'wb') as output:\n",
    "        pickle.dump(obj, output)\n",
    "\n",
    "    print('objects saved for epoch: {}'.format(epoch))\n",
    "\n",
    "\n",
    "def restore_model(model, out_path):\n",
    "    chk_file = glob.glob(out_path + '/' + '*.pth')\n",
    "\n",
    "    if chk_file:\n",
    "        chk_file = str(chk_file[0])\n",
    "        print('found modeL {}, restoring'.format(chk_file))\n",
    "        model.load_state_dict(torch.load(chk_file))\n",
    "    else:\n",
    "        print('Model not found, using untrained model')\n",
    "    return model\n",
    "\n",
    "\n",
    "def restore_objects(out_path, default):\n",
    "    data_file = glob.glob(out_path + '/' + '*.dat')\n",
    "    if data_file:\n",
    "        data_file = str(data_file[0])\n",
    "        print('found data {}, restoring'.format(data_file))\n",
    "        with open(data_file, 'rb') as input_:\n",
    "            obj = pickle.load(input_)\n",
    "\n",
    "        return obj\n",
    "    else:\n",
    "        return default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics(pred: torch.Tensor, label: torch.Tensor, metric_dict: dict):\n",
    "    metric_dict['accuracy'] += torch.sum((pred == label)).item()\n",
    "    metric_dict['true_pos'] += torch.sum((label == 1) & (pred == 1)).item()\n",
    "    metric_dict['true_neg'] += torch.sum((label == 0) & (pred == 0)).item()\n",
    "    metric_dict['false_pos'] += torch.sum((label == 0) & (pred == 1)).item()\n",
    "    metric_dict['false_neg'] += torch.sum((label == 1) & (pred == 0)).item()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    metric_dict = {\n",
    "        'accuracy': 0,\n",
    "        'true_pos': 0,\n",
    "        'true_neg': 0,\n",
    "        'false_pos': 0,\n",
    "        'false_neg': 0\n",
    "    }\n",
    "\n",
    "    for batch_idx, ((mfcc, non_mfcc, path), label) in enumerate(tqdm.tqdm(train_loader)):\n",
    "        mfcc, non_mfcc, label = mfcc.to(device), non_mfcc.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(mfcc, non_mfcc)\n",
    "        loss = model.loss(out, label)\n",
    "        with torch.no_grad():\n",
    "            prob = torch.nn.functional.softmax(out, dim=1)\n",
    "            pred = torch.argmax(prob, dim=1)\n",
    "            update_metrics(pred=pred, label=label, metric_dict=metric_dict)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                time.ctime(time.time()),\n",
    "                epoch, batch_idx * len(mfcc), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    accuracy_mean = (100. * metric_dict['accuracy']) / len(train_loader.dataset)\n",
    "\n",
    "    metric_dict['batch_losses'] = losses\n",
    "    metric_dict['accuracy_mean'] = accuracy_mean\n",
    "    metric_dict['precision'] = (metric_dict[\"true_pos\"]) / (metric_dict[\"true_pos\"] + metric_dict[\"false_pos\"])\n",
    "    metric_dict['recall'] = (metric_dict[\"true_pos\"]) / (metric_dict[\"true_pos\"] + metric_dict[\"false_neg\"])\n",
    "    metric_dict['f1_score'] = (2.0 * metric_dict['precision'] * metric_dict['recall']) / \\\n",
    "                              (metric_dict['precision'] + metric_dict['recall'])\n",
    "\n",
    "    return np.mean(losses), accuracy_mean, metric_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, log_interval=None):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    metric_dict = {\n",
    "        'accuracy': 0,\n",
    "        'true_pos': 0,\n",
    "        'true_neg': 0,\n",
    "        'false_pos': 0,\n",
    "        'false_neg': 0\n",
    "    }\n",
    "\n",
    "    data_check_dict = {'path': [], 'label': [], 'pred': [], 'prob_0': [], 'prob_1': []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, ((mfcc, non_mfcc, path), label) in enumerate(tqdm.tqdm(test_loader)):\n",
    "            mfcc, non_mfcc, label = mfcc.to(device), non_mfcc.to(device), label.to(device)\n",
    "            out = model(mfcc, non_mfcc)\n",
    "            prob = torch.nn.functional.softmax(out, dim=1)\n",
    "            test_loss_on = model.loss(out, label).item()\n",
    "            losses.append(test_loss_on)\n",
    "\n",
    "            pred = torch.argmax(prob, dim=1)\n",
    "            update_metrics(pred=pred, label=label, metric_dict=metric_dict)\n",
    "\n",
    "            if log_interval is not None and batch_idx % log_interval == 0:\n",
    "                print('{} Test: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    time.ctime(time.time()),\n",
    "                    batch_idx * len(mfcc), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader), test_loss_on))\n",
    "\n",
    "            data_check_dict['path'] += path\n",
    "            data_check_dict['label'] += label.tolist()\n",
    "            data_check_dict['pred'] += pred.tolist()\n",
    "            data_check_dict['prob_0'] += prob[:, 0].tolist()\n",
    "            data_check_dict['prob_1'] += prob[:, 1].tolist()\n",
    "\n",
    "    data_check_df = pd.DataFrame(data_check_dict)\n",
    "    data_check_df.to_csv('data_check_test.csv', index=False)\n",
    "\n",
    "    test_loss = np.mean(losses)\n",
    "    accuracy_mean = (100. * metric_dict['accuracy']) / len(test_loader.dataset)\n",
    "\n",
    "    metric_dict['batch_losses'] = losses\n",
    "    metric_dict['accuracy_mean'] = accuracy_mean\n",
    "    metric_dict['precision'] = (metric_dict[\"true_pos\"]) / (metric_dict[\"true_pos\"] + metric_dict[\"false_pos\"])\n",
    "    metric_dict['recall'] = (metric_dict[\"true_pos\"]) / (metric_dict[\"true_pos\"] + metric_dict[\"false_neg\"])\n",
    "    metric_dict['f1_score'] = (2.0 * metric_dict['precision'] * metric_dict['recall']) / \\\n",
    "                              (metric_dict['precision'] + metric_dict['recall'])\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{}, ({:.4f})%\\n'.format(\n",
    "        test_loss, metric_dict['accuracy'], len(test_loader.dataset), accuracy_mean))\n",
    "\n",
    "    return test_loss, accuracy_mean, metric_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # needs three command line arguments\n",
    "    # 1. root path of train data\n",
    "    # 2. root path of test data\n",
    "    # 3. path where saved models are saved\n",
    "    # 4. Learning rate\n",
    "    # 5. Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/arunav/Desktop/8th-semester/RE/lexical-stress-detection-master/dev-clean/train_data\"\n",
    "test_path = \"/home/arunav/Desktop/8th-semester/RE/lexical-stress-detection-master/dev-clean/test_data\"\n",
    "model_path =\"/home/arunav/Desktop/8th-semester/RE/lexical-stress-detection-master/models\"\n",
    "learning_rate =0.01\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_path, test_path, model_path, learning_rate, epochs):\n",
    "    print('train path: {}'.format(train_path))\n",
    "    print('test path: {}'.format(test_path))\n",
    "    print('model path: {}'.format(model_path))\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    torch.cuda.current_device()\n",
    "    print('using device', device)\n",
    "\n",
    "    import multiprocessing\n",
    "    print('num cpus:', multiprocessing.cpu_count())\n",
    "\n",
    "    kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
    "              'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    train_dataset = CNNDataset(root=train_path)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, **kwargs)\n",
    "\n",
    "    test_dataset = CNNDataset(root=test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=True, **kwargs)\n",
    "\n",
    "    print('Folder to Index: {}'.format(train_dataset.folder_to_index))\n",
    "\n",
    "    model = CNNStressNet(reduction='mean').to(device)\n",
    "    model = restore_model(model, model_path)\n",
    "    last_epoch, max_accuracy, train_losses, test_losses, all_train_metrics, all_test_metrics = \\\n",
    "        restore_objects(model_path, (0, 0, [], [], [], []))\n",
    "\n",
    "    start = last_epoch + 1 if max_accuracy > 0 else 0\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    test_loss, test_accuracy, test_metrics = test(model, device, test_loader)\n",
    "    print('Before any training:, test loss is: {}, test_accuracy: {}'.format(test_loss, test_accuracy))\n",
    "\n",
    "    for epoch in range(start, start + epochs):\n",
    "        train_loss, train_accuracy, train_metrics = train(model, device, train_loader, optimizer, epoch, 250)\n",
    "        test_loss, test_accuracy, test_metrics = test(model, device, test_loader)\n",
    "        print('After epoch: {}, train_loss: {}, test loss is: {}, train_accuracy: {}, test_accuracy: {}'.format(\n",
    "            epoch, train_loss, test_loss, train_accuracy, test_accuracy))\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        all_train_metrics.append(train_metrics)\n",
    "        all_test_metrics.append(test_metrics)\n",
    "\n",
    "        if test_accuracy > max_accuracy:\n",
    "            max_accuracy = test_accuracy\n",
    "            save_model(model, epoch, model_path)\n",
    "            save_objects((epoch, max_accuracy, train_losses, test_losses, all_train_metrics, all_test_metrics),\n",
    "                         epoch, model_path)\n",
    "            print('saved epoch: {} as checkpoint'.format(epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train path: /home/arunav/Desktop/8th-semester/RE/lexical-stress-detection-master/dev-clean/train_data\n",
      "test path: /home/arunav/Desktop/8th-semester/RE/lexical-stress-detection-master/dev-clean/test_data\n",
      "model path: /home/arunav/Desktop/8th-semester/RE/lexical-stress-detection-master/models\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-98c6d0ec53b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-eedb15e5b2cd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_path, test_path, model_path, learning_rate, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0muse_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'using device'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    196\u001b[0m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0m_cudart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50"
     ]
    }
   ],
   "source": [
    "main(train_path, test_path, model_path, learning_rate, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
